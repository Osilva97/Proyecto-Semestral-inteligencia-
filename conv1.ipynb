{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "conv1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hI8tV5uVHZX"
      },
      "source": [
        "### [Google Colab] subir código cifar10.py. Basta con ejecutar una vez (inmune a reinicios de kernel de python, no de la máquina)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7EKoANMPBps",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0550bb57-e22e-4337-cca8-bdfe8800d436"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-100d62fe-4a8a-4b3b-b8ef-44dc1011db2c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-100d62fe-4a8a-4b3b-b8ef-44dc1011db2c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cifar10.py to cifar10 (1).py\n",
            "User uploaded file \"cifar10.py\" with length 6703 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgIK9vPiSHU1"
      },
      "source": [
        "# Aquí comienza el cuerpo del código\n",
        "## Reinicie luego de entrenar cada modelo. Recuerde actualizar el directorio de tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFrRwKUHOhs-"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from cifar10 import CIFAR10"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYzZFJpM08iZ"
      },
      "source": [
        "### Constructor del dataset. Modifique aquí el *flag* de *data augmentation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hk74ROkOhtO"
      },
      "source": [
        "# Load dataset\n",
        "batch_size = 64\n",
        "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCAGDv5M0hFU"
      },
      "source": [
        "## Elija aquí el directorio donde guardará los registros de Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqn2fJpRUYkr"
      },
      "source": [
        "PARENT_DIR = './summaries/'\n",
        "SUMMARIES_DIR = PARENT_DIR + 'mlp'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaGDU7CF0p71"
      },
      "source": [
        "## Constructores de modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTXZBRJCswCE"
      },
      "source": [
        "# Modelo de red convolucional\n",
        "class ConvClassifier(tf.keras.Model):\n",
        "    \"\"\"Implementacion de clasificador en base a red neuronal convolucional.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        layer_sizes,\n",
        "        learning_rate=0.0005,\n",
        "        dropout_rate=0.5):\n",
        "        \"\"\"Construye un clasificador Red Neuronal Convolucional.\n",
        "        \n",
        "        Args:\n",
        "            input_shape: tupla que contiene dimensiones de las imagenes de entrada\n",
        "                al modelo, exluyendo la dimensióón de batch. La tupla ha de tener largo 3\n",
        "                y formato (Image_height, image_width, Channels)\n",
        "            layer_sizes: Lista de enteros que indica la cantidad de filtros \n",
        "                convolucionales en cada capa de la red.\n",
        "            learning_rate: Escalar que indica la tasa de aprendizaje.\n",
        "            dropout_rate: Probabilidad con la que se apaga cada neurona de la \n",
        "                capa fully connected 1 durante el entrenamiento del modelo.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Inicializa atributos propios de un objeto del tipo tf.keras.Model\n",
        "        super().__init__()\n",
        "        # Agregar parametros al objeto\n",
        "        self.input_shape_tuple = input_shape\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "        # Inicializa capas del modelo\n",
        "        self.layers_list = self._init_layers(layer_sizes)\n",
        "        # Crea un objeto optimizador\n",
        "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
        "        # Crea un objeto funcion de costo-perdida\n",
        "        self.loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_function_name = 'xentropy'\n",
        "\n",
        "    def _init_layers(self, layer_sizes):\n",
        "        \"\"\"Inicializa-crea los objetos de cada capa de la red CNN.\n",
        "\n",
        "        Args:\n",
        "        layer_sizes: Lista de enteros que indica la cantidad de filtros convolucionales\n",
        "        de cada capa convolucional.\n",
        "\n",
        "        Returns:\n",
        "        layers_list: Lista de objetos asociados a cada capa creada.\n",
        "        \"\"\"   \n",
        "\n",
        "        layers_list = []\n",
        "        # Inicializacion de capas del modelo\n",
        "        # Inicializacion de capas de entrada, que fija el tamaño de las entradas\n",
        "        layers_list.append(tf.keras.layers.InputLayer((self.input_shape_tuple)))\n",
        "        # Inicializacion de capas convolucionales\n",
        "\n",
        "        ###########################RESIZE IMAGENES############################\n",
        "\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.Conv2D(\n",
        "              16, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv_primera_capa' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              32, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv2' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              64, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv3' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              64, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv4' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              64, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv5' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              64, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv6' ))\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.MaxPool2D())\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              128, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv7' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              128, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv8' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "              128, kernel_size=3, padding='same', \n",
        "              bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "              activation=tf.nn.relu, name='conv9' ))\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.MaxPool2D())\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "                256, kernel_size=3, padding='same', \n",
        "                bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "                activation=tf.nn.relu, name='conv10' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "                256, kernel_size=3, padding='same', \n",
        "                bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "                activation=tf.nn.relu, name='conv11' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "                256, kernel_size=3, padding='same', \n",
        "                bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "                activation=tf.nn.relu, name='conv12' ))\n",
        "        layers_list.append(\n",
        "          tf.keras.layers.Conv2D(\n",
        "                256, kernel_size=3, padding='same', \n",
        "                bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
        "                activation=tf.nn.relu, name='conv13' ))\n",
        "        # Inicializacion de funcion de capa de pooling\n",
        "       \n",
        "        # Inicializacion de capa que aplana featuremaps de la convolucion para que \n",
        "        # puedan entrar a capas fully-connected\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.Flatten())\n",
        "        # Inicializacion de capa fully-connected 1\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.Dense(50, activation=tf.nn.relu, name='fc1'))\n",
        "        # Inicializacion de capa dropout\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.Dropout(self.dropout_rate))\n",
        "        # Inicializacion de capa fully-connected 2\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.Dense(10, name='fc2'))\n",
        "        # Inicializacion de activacion de salida\n",
        "        layers_list.append(\n",
        "            tf.keras.layers.Activation(tf.nn.softmax))\n",
        "        return layers_list\n",
        "\n",
        "    def call(self, x, training=False, get_logits=False):\n",
        "        \"\"\"Metodo que entrega la salida del modelo, \n",
        "        define el paso forward de la red neuronal, relacionando las capas \n",
        "        de la red entre si. Se define la arquitectura del modelo.\n",
        "\n",
        "        Args:\n",
        "          x: Tensor de entrada de dimensiones (batch_size, n_features).\n",
        "          training: Boolean que indica si se esta en fase de entrenamiento o no.\n",
        "            En este caso define el comportamiento durante entrenamiento y evaluacion de capas \n",
        "             como dropout o batch normalization \n",
        "          get_logits: Bolean que indica si como salida del modelo se obtienen los\n",
        "            logits o las predicciones\n",
        "\n",
        "        Returns:\n",
        "          x: Tensor de salida de la red, de dimensiones (batch_size, n_classes).\n",
        "        \"\"\"\n",
        "        for layer_index, layer in enumerate(self.layers_list):\n",
        "            # se se desean los logits, se retorna la salida del modelo sin la ultima activacion\n",
        "            if get_logits and layer_index == (len(self.layers_list)-1):\n",
        "                return x \n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, x_data, y_labels):\n",
        "        \"\"\"Metodo que realiza la actualizacion por gradiente.\n",
        "\n",
        "        Se aplica un algoritmo de optimizacion para ejecutar una\n",
        "        iteracion de minimizacion por gradiente sobre el loss del modelo.\n",
        "\n",
        "        Args:\n",
        "        x_data: datos de entrenamientos sobre los que calcular la loss.\n",
        "        y_labels: etiquetas de entrenamiento sobre las que calcular la loss.\n",
        "\n",
        "        Returns:\n",
        "        loss: el valor de la funcion de costo para x_data, y_labels.\n",
        "        accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "        # training=True is only needed if there are layers with different\n",
        "        # behavior during training versus inference (e.g. Dropout).\n",
        "            logits = self.call(x_data, training=True, get_logits=True)\n",
        "            loss = self.loss_object(y_labels, logits)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def _get_accuracy_from_logits(self, logits, labels):\n",
        "        \"\"\"Metodo que entrega el accuracy entre logits de prediccion y etiquetas.\n",
        "\n",
        "        Args:\n",
        "          logits: para datos de etiqueta labels.\n",
        "          labels: etiquetas.\n",
        "\n",
        "        Returns:\n",
        "          accuracy: porcentaje de aciertos por los logits, respecto a los labels.\n",
        "        \"\"\"\n",
        "        softmax_prediction = tf.nn.softmax(logits)\n",
        "        correct_prediction = tf.equal(tf.argmax(softmax_prediction, 1),\n",
        "                             tf.argmax(labels, 1))\n",
        "        accuracy = tf.reduce_mean(\n",
        "            tf.cast(correct_prediction, tf.float32),\n",
        "            name='accuracy')\n",
        "        return accuracy\n",
        "\n",
        "    @tf.function\n",
        "    def eval_step(self, x_data, y_labels):\n",
        "        \"\"\"Metodo que evalua la funcion de costo y el accuracy del modelo.\n",
        "\n",
        "        Args:\n",
        "        x_data: datos sobre los que calcular la loss y accuracy.\n",
        "        y_labels: etiquetas sobre las que calcular la loss y accuracy.\n",
        "\n",
        "        Returns:\n",
        "        loss: el valor de la funcion de costo para x_data, y_labels.\n",
        "        accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
        "        \"\"\"\n",
        "        logits = self.call(x_data, training=False, get_logits=True)\n",
        "        loss = self.loss_object(y_labels, logits)\n",
        "        accuracy = self._get_accuracy_from_logits(logits, y_labels)\n",
        "        return loss, accuracy\n",
        "\n",
        "    @tf.function\n",
        "    def prediction_step(self, x_data):\n",
        "        \"\"\"Metodo que retorna predicciones para x_data\n",
        "\n",
        "        Args:\n",
        "          x_data: datos a predecir.\n",
        "\n",
        "        Returns:\n",
        "          predictions: predicciones para x_data\n",
        "        \"\"\"\n",
        "        predictions = self.call(x_data, training=False, get_logits=False)\n",
        "        return predictions\n",
        "\n",
        "    def write_to_evaluation_summary(self, summary_writer, step, loss, accuracy):\n",
        "        \"\"\"Metodo que calcula metricas y parametros a ser monitoreados en tensorboard.\n",
        "        Monitorea histograma de parametros, metricas del modelo e imagenes de filtros\n",
        "        de primera capa convolucional. Para datos de evaluacion\n",
        "\n",
        "        Args:\n",
        "          summary_writer: Summary writer de evaluacion\n",
        "          step: iteracion del evaluacion\n",
        "          loss: valor de la funcion de costo para datos de evaluacion\n",
        "          accuracy: accuracy para datos de evaluacion\n",
        "        \"\"\"\n",
        "        with summary_writer.as_default():\n",
        "            for layer_i in self.layers_list:\n",
        "                layer_name = layer_i.name\n",
        "                if 'fc' in layer_name or 'conv' in layer_name:\n",
        "                    weights, biases = layer_i.get_weights()\n",
        "                    tf.summary.histogram(layer_name + '/weights', weights, step=step)\n",
        "                    tf.summary.histogram(layer_name + '/biases', biases, step=step)\n",
        "                if 'conv_primera_capa' in layer_name:\n",
        "                    tf.summary.image(\n",
        "                        'conv1_filters',\n",
        "                        tf.transpose(weights, perm=[3, 0, 1, 2]), step=step,\n",
        "                        max_outputs=self.layer_sizes[0]\n",
        "                    )\n",
        "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
        "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
        "\n",
        "    def write_to_learning_summary(self, summary_writer, step, loss, accuracy):\n",
        "        \"\"\"Metodo que calcula metricas a ser monitoreados en tensorboard.\n",
        "        Monitorea metricas del modelo.\n",
        "\n",
        "        Args:\n",
        "          summary_writer: Summary writer de\n",
        "          step: iteracion\n",
        "          loss: valor de la funcion de costo para datos\n",
        "          accuracy: accuracy para datos\n",
        "        \"\"\"\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
        "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
        "    \n",
        "\n",
        "    def write_graph_to_summary(self, summary_writer, X_data, logdir):\n",
        "        \"\"\"Metodo que guarda el grafo de evaluacion (arquitectura del modelo)\n",
        "        en tensorboard. Se guardan las operaciones involugradas en un paso forward\n",
        "        o self.call del modelo\n",
        "\n",
        "        Args:\n",
        "          summary_writer: Summary writer para datos\n",
        "          X_data: Datos\n",
        "        \"\"\"\n",
        "        logdir = logdir + '/graph'\n",
        "        tf.summary.trace_on(graph=True, profiler=False)\n",
        "        predicted_proba = self.prediction_step(X_data)\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.trace_export(\n",
        "                name=\"prediction_step\",\n",
        "                step=0,\n",
        "                profiler_outdir=logdir)\n",
        "        tf.summary.trace_off()\n",
        "    \n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Retorna las probabilidades de clase para los datos de entrada.\n",
        "              \n",
        "        Args:\n",
        "            X: datos de entrada\n",
        "        \n",
        "        Returns:\n",
        "            predicted_proba.numpy(): Arreglo numpy de las probabilidades de \n",
        "              cada clase, para cada muestra de X.\n",
        "        \"\"\"\n",
        "        # Obtener las probabilidades de salida de cada clase\n",
        "        predicted_proba = self.prediction_step(X)\n",
        "        return predicted_proba.numpy()\n",
        "    \n",
        "    def predict_label(self, X):\n",
        "        \"\"\"Retorna la etiqueta predicha para los datos de entrada.\n",
        "        \n",
        "        Args:\n",
        "            X: datos de entrada\n",
        "        \n",
        "        Returns:\n",
        "            predicted_labels: Arreglo numpy de las predicciones de cada muestra de X.\n",
        "        \"\"\"\n",
        "        # Obtener la probabilidad de cada clase\n",
        "        predicted_proba = self.prediction_step(X)\n",
        "        # Etiquetar segun la etiqueta mas probable\n",
        "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
        "        return predicted_labels"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBFYJU6uPGLB"
      },
      "source": [
        "\n",
        "# Modelo de red MLP\n",
        "class MLPClassifier(ConvClassifier):\n",
        "    \"\"\"Implementacion de clasificador en base a red MLP.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=0.0005,\n",
        "        dropout_rate=0.5):\n",
        "        \"\"\"Construye un clasificador Perceptron Multicapa de 2 capas.\n",
        "        \n",
        "        Args:\n",
        "            learning_rate: Escalar que indica la tasa de aprendizaje.\n",
        "            dropout_rate: Probabilidad con la que se apaga cada neurona de la \n",
        "                capa fully connected 1 durante el entrenamiento del modelo.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Inicializa atributos propios de un objeto del tipo tf.keras.Model\n",
        "        tf.keras.Model.__init__(self)\n",
        "        # Agregar parametros al objeto\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "        # Inicializa capas del modelo\n",
        "        self.layers_list = self._init_layers()\n",
        "        # Crea un objeto optimizador\n",
        "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
        "        # Crea un objeto funcion de costo-perdida\n",
        "        self.loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_function_name = 'xentropy'\n",
        "    \n",
        "    def _init_layers(self):\n",
        "        \"\"\"Inicializa-crea los objetos de cada capa de la red MLP.\n",
        "\n",
        "        Returns:\n",
        "          layers_list: Lista de objetos asociados a cada capa creada.\n",
        "        \"\"\"\n",
        "        layers_list = []\n",
        "        # Inicializacion de capas del modelo\n",
        "        # Inicializacion de capas de entrada, que fija el tamaño de las entradas\n",
        "        layers_list.append(tf.keras.layers.Flatten())\n",
        "        # Inicializacion de capas convolucionales\n",
        "        layers_list.append(tf.keras.layers.Dense(100, activation=tf.nn.relu, name='fc1'))\n",
        "        # Inicializacion de capa dropout\n",
        "        layers_list.append(tf.keras.layers.Dropout(self.dropout_rate))\n",
        "        # Inicializacion de capa fully-connected 2\n",
        "        layers_list.append(tf.keras.layers.Dense(10, name='fc2'))\n",
        "        # Inicializacion de activacion de salida\n",
        "        layers_list.append(tf.keras.layers.Activation(tf.nn.softmax))\n",
        "        return layers_list"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvYNrw25Vi__"
      },
      "source": [
        "## Inicializacion de modelos\n",
        "\n",
        "### Aqui puede escoger que modelo usar y modificar el valor de *dropout_rate* usado al computar *train_step* para activar/desactivar el *dropout_rate*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y0ezm__OhtY"
      },
      "source": [
        "#### PARAMETROS DE MODELOS A MODIFICAR ###\n",
        "use_convnet = True\n",
        "n_filters_convs = [16, 32, 64, 64 ,64 ,64 ,128 ,128 ,128, 128, 128, 256,256,256,256,256]\n",
        "dropout_rate = 0.5\n",
        "\n",
        "if use_convnet:\n",
        "    model = ConvClassifier(\n",
        "        input_shape=(1000, 1000, 3),\n",
        "        layer_sizes=n_filters_convs,\n",
        "        dropout_rate=dropout_rate,\n",
        "        learning_rate=0.0005)\n",
        "\n",
        "else:\n",
        "    model = MLPClassifier(\n",
        "        dropout_rate=dropout_rate,\n",
        "        learning_rate=0.0005)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAIrQw7r2E47"
      },
      "source": [
        "### Construcción del optimizador + funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMUAQYlOhtc"
      },
      "source": [
        "# Useful training functions\n",
        "def validate(model, val_summary_writer, step):\n",
        "    cifar10.shuffleValidation()\n",
        "    batches = cifar10.getValidationSet(asBatches=True)\n",
        "    accs = []\n",
        "    xent_vals = []\n",
        "    for batch in batches:\n",
        "        data, labels = batch\n",
        "        xentropy_val, acc = model.eval_step(data, labels)\n",
        "        accs.append(acc)\n",
        "        xent_vals.append(xentropy_val)\n",
        "    mean_xent = np.array(xent_vals).mean()    \n",
        "    mean_acc = np.array(accs).mean()\n",
        "    model.write_to_evaluation_summary(val_summary_writer, step, mean_xent, mean_acc)\n",
        "    return mean_acc, mean_xent\n",
        "\n",
        "def test(model):\n",
        "    batches = cifar10.getTestSet(asBatches=True)\n",
        "    accs = []\n",
        "    for batch in batches:\n",
        "        data, labels = batch\n",
        "        _, acc = model.eval_step(data, labels)\n",
        "        accs.append(acc)\n",
        "    mean_acc = np.array(accs).mean()\n",
        "    return mean_acc\n",
        "\n",
        "# Tensorboard writers\n",
        "train_writer = tf.summary.create_file_writer(SUMMARIES_DIR+'/train')\n",
        "model.write_graph_to_summary(train_writer, cifar10.getValidationSet()[0], SUMMARIES_DIR)\n",
        "validation_writer = tf.summary.create_file_writer(SUMMARIES_DIR+'/validation')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUc5-KOHSALR"
      },
      "source": [
        "## Entrenar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRKMxMztOhtg",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4592ae9e-a388-4ceb-e463-d8f71c9a05a8"
      },
      "source": [
        "cifar10.reset()\n",
        "print(\"Trainable variables\")\n",
        "for n in model.trainable_weights:\n",
        "    print(n.name, ' ', n.shape)\n",
        "if use_convnet:\n",
        "    epochs = 50\n",
        "else:\n",
        "    epochs = 50\n",
        "    \n",
        "t_i = time.time()\n",
        "n_batches = cifar10.n_batches\n",
        "val_acc_vals = []\n",
        "test_acc_vals = []\n",
        "while cifar10.getEpoch() < epochs:\n",
        "    epoch = cifar10.getEpoch()\n",
        "    batch, batch_idx = cifar10.nextBatch()\n",
        "    batch_data = batch[0]\n",
        "    batch_labels = batch[1]\n",
        "    \n",
        "    # just a training iteration\n",
        "    model.train_step(batch_data, batch_labels)\n",
        "    \n",
        "    step = batch_idx+epoch*n_batches\n",
        "    \n",
        "    # Write training summary\n",
        "    if step % 50 == 0:\n",
        "        loss, accuracy = model.eval_step(batch_data, batch_labels)\n",
        "        model.write_to_learning_summary(train_writer, step, loss, accuracy)\n",
        "        \n",
        "    # gradient (by layer) statistics over last training batch & validation summary\n",
        "    if batch_idx==0:\n",
        "        loss, acc = model.eval_step(batch_data, batch_labels)\n",
        "        validation_accuracy, validation_loss = validate(model, validation_writer, step)\n",
        "        print('[Epoch %d, it %d] Training acc. %.3f, loss %.3f. \\\n",
        "Valid. acc. %.3f, loss %.3f' % (\n",
        "            epoch,\n",
        "            step,\n",
        "            acc,\n",
        "            loss,\n",
        "            validation_accuracy,\n",
        "            validation_loss\n",
        "        ))\n",
        "        val_acc_vals.append(validation_accuracy)\n",
        "        test_accuracy = test(model)\n",
        "        test_acc_vals.append(test_accuracy)\n",
        "        print(\"Time elapsed %.2f minutes\" % ((time.time()-t_i)/60.0))\n",
        "\n",
        "val_acc_vals = np.array(val_acc_vals)\n",
        "test_acc_vals = np.array(test_acc_vals)\n",
        "best_epoch = np.argmax(val_acc_vals)\n",
        "test_acc_at_best = test_acc_vals[best_epoch]\n",
        "print('*'*30)\n",
        "print(\"Testing set accuracy @ epoch %d (best validation acc): %.4f\" % (best_epoch, test_acc_at_best))\n",
        "print('*'*30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable variables\n",
            "conv_primera_capa/kernel:0   (3, 3, 3, 16)\n",
            "conv_primera_capa/bias:0   (16,)\n",
            "conv2/kernel:0   (3, 3, 16, 32)\n",
            "conv2/bias:0   (32,)\n",
            "conv3/kernel:0   (3, 3, 32, 64)\n",
            "conv3/bias:0   (64,)\n",
            "conv4/kernel:0   (3, 3, 64, 64)\n",
            "conv4/bias:0   (64,)\n",
            "conv5/kernel:0   (3, 3, 64, 64)\n",
            "conv5/bias:0   (64,)\n",
            "conv6/kernel:0   (3, 3, 64, 64)\n",
            "conv6/bias:0   (64,)\n",
            "conv7/kernel:0   (3, 3, 64, 128)\n",
            "conv7/bias:0   (128,)\n",
            "conv8/kernel:0   (3, 3, 128, 128)\n",
            "conv8/bias:0   (128,)\n",
            "conv9/kernel:0   (3, 3, 128, 128)\n",
            "conv9/bias:0   (128,)\n",
            "conv10/kernel:0   (3, 3, 128, 256)\n",
            "conv10/bias:0   (256,)\n",
            "conv11/kernel:0   (3, 3, 256, 256)\n",
            "conv11/bias:0   (256,)\n",
            "conv12/kernel:0   (3, 3, 256, 256)\n",
            "conv12/bias:0   (256,)\n",
            "conv13/kernel:0   (3, 3, 256, 256)\n",
            "conv13/bias:0   (256,)\n",
            "fc1/kernel:0   (16384, 50)\n",
            "fc1/bias:0   (50,)\n",
            "fc2/kernel:0   (50, 10)\n",
            "fc2/bias:0   (10,)\n",
            "[Epoch 0, it 0] Training acc. 0.141, loss 5.166. Valid. acc. 0.095, loss 6.954\n",
            "Time elapsed 0.12 minutes\n",
            "[Epoch 1, it 703] Training acc. 0.047, loss 2.303. Valid. acc. 0.096, loss 2.303\n",
            "Time elapsed 1.02 minutes\n",
            "[Epoch 2, it 1406] Training acc. 0.172, loss 2.302. Valid. acc. 0.095, loss 2.303\n",
            "Time elapsed 1.91 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h554rbJcR7Jj"
      },
      "source": [
        "## Visualizar en Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL3_pq6eOhtl"
      },
      "source": [
        "LOG_DIR = PARENT_DIR  # este es el logdir de nuestros summaries\n",
        "print(\"Showing summaries at %s\" % (LOG_DIR))\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='$LOG_DIR'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUYkbGcHT3Vz"
      },
      "source": [
        "%%bash\n",
        "ls\n",
        "ps aux | grep -e 'tensorboard'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRtK8EiGMJ9n"
      },
      "source": [
        "#!rm -r summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi-pkaRAN7Yf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpmvQZn8HcLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}